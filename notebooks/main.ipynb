{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv\n",
    "import pathlib as pb\n",
    "import sys\n",
    "\n",
    "\n",
    "# Add `ape` package to SYS path\n",
    "sys.path.append(str(pb.Path(find_dotenv()).parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, cast\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import evaluate as eval\n",
    "import lightning as lit\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from dataclasses import asdict\n",
    "from transformers import AlbertTokenizer\n",
    "from collections import defaultdict, namedtuple\n",
    "from transformers import AutoModel, AutoTokenizer, XLMRobertaForCausalLM\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onmt.onmt.modules.position_ffn import ActivationFunction\n",
    "from onmt.onmt.translate.translator import GeneratorLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ape\n",
    "from ape.data.types import DataSplit\n",
    "from ape.data.types import APETripletDict\n",
    "from ape.eval.metrics import APEMetrics\n",
    "from ape.data.dataset import APEDataset\n",
    "from ape.data.transform import Tokenize, HFTokenizer\n",
    "from ape.model.mst import MultiSourceTransformerCausalLM\n",
    "from ape.model.encoders import MultiSourceTransformerEncoder\n",
    "from ape.model.decoders import MultiSourceTransformerDecoder\n",
    "from ape.light.causal_lm import MultiSourceCausalLMLightningModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TER, chrF and BLEU\n",
    "metrics = APEMetrics(cache_dir=ape.HF_CACHE_DIR)\n",
    "\n",
    "# Load APE Dataset (Original + Synthetic)\n",
    "ds_test = APEDataset(path=ape.DATA_DIR, split='test')\n",
    "ds_train = APEDataset(path=ape.DATA_DIR, split='train')\n",
    "ds_train, ds_valid = random_split(ds_train, lengths=[0.99, 0.01], generator=ape.gen_torch)\n",
    "\n",
    "# Aggregate all subsets into a single object\n",
    "ds: Dict[DataSplit, Dataset[APETripletDict[str]]] = {\n",
    "    'train': ds_train,\n",
    "    'valid': ds_valid,\n",
    "    'test': ds_test,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose encoders for each input type\n",
    "encoder_type_src = 'roberta-base'\n",
    "encoder_type_mt = 'l3cube-pune/marathi-roberta'\n",
    "\n",
    "# Load source and target tokenizers\n",
    "hf_tokenizer_src = AutoTokenizer.from_pretrained(encoder_type_src,\n",
    "                                                                                 use_fast=True,\n",
    "                                                                                 padding_side='right',\n",
    "                                                                                 truncation_side='right',\n",
    "                                                                                 cache_dir=ape.HF_CACHE_DIR / 'tokenizers')\n",
    "hf_tokenizer_mt = AutoTokenizer.from_pretrained(encoder_type_mt,\n",
    "                                                                                use_fast=True,\n",
    "                                                                                padding_side='right',\n",
    "                                                                                truncation_side='right',\n",
    "                                                                                cache_dir=ape.HF_CACHE_DIR / 'tokenizers')\n",
    "\n",
    "# Wrap and customize HF Tokenizers\n",
    "max_seq_len = 512\n",
    "tokenizer_src = HFTokenizer(hf_tokenizer_src, source_prefix='src', max_length=max_seq_len)\n",
    "tokenizer_mt = HFTokenizer(hf_tokenizer_mt, source_prefix='mt', max_length=max_seq_len)\n",
    "tokenizer_pe = HFTokenizer(hf_tokenizer_mt, source_prefix='pe', max_length=max_seq_len)\n",
    "tokenize = Tokenize([tokenizer_src, tokenizer_mt, tokenizer_pe])\n",
    "\n",
    "# Use same settings across all splits\n",
    "DefaultDataLoader = partial(DataLoader,\n",
    "                            collate_fn=tokenize,\n",
    "                            num_workers=ape.WORKERS,\n",
    "                            batch_size=ape.BATCH_SIZE,\n",
    "                            prefetch_factor=ape.PREFETCH_FACTOR)\n",
    "\n",
    "# Aggregate all dataloaders into a single object\n",
    "dl: Dict[DataSplit, DataLoader] = {\n",
    "    'train': DefaultDataLoader(ds['train']),\n",
    "    'valid': DefaultDataLoader(ds['valid']),\n",
    "    'test': DefaultDataLoader(ds['test']),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = MultiSourceCausalLMLightningModule(\n",
    "    bos_token_id=hf_tokenizer_mt.bos_token_id,\n",
    "    encoder_type_src=encoder_type_src,\n",
    "    encoder_type_mt=encoder_type_mt,\n",
    "    tokenizer_mt=hf_tokenizer_mt,\n",
    "    block_size=max_seq_len,\n",
    "    temperature=1.0,\n",
    "    do_sample=True,\n",
    "    top_k=8,\n",
    ")\n",
    "\n",
    "logger = MLFlowLogger(\n",
    "    experiment_name='Automatic Post-Editing',\n",
    "    tracking_uri=ape.MLFLOW_TRACKING_URI,\n",
    "    tags={ 'test': 'true', },\n",
    ")\n",
    "\n",
    "trainer = lit.Trainer(\n",
    "    logger=logger,\n",
    "    limit_val_batches=200,\n",
    "    val_check_interval=5_000,\n",
    "    accumulate_grad_batches=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invokariman/.cache/pypoetry/virtualenvs/ape-zcZ_0igR-py3.11/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                           | Params\n",
      "---------------------------------------------------------\n",
      "0 | model | MultiSourceTransformerCausalLM | 629 M \n",
      "---------------------------------------------------------\n",
      "629 M     Trainable params\n",
      "0         Non-trainable params\n",
      "629 M     Total params\n",
      "2,519.085 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634a19d311794014a64b075cfca09875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invokariman/.cache/pypoetry/virtualenvs/ape-zcZ_0igR-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/home/invokariman/.cache/pypoetry/virtualenvs/ape-zcZ_0igR-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bac4f562355466ba1a78fe13c3602de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=dl['train'], val_dataloaders=dl['valid'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ub-g21-mt-zcZ_0igR-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
